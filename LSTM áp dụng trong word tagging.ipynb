{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Mô hình chuỗi và Long-Short Term Memory Networks\n",
    "\n",
    "Trong LSTM,  ứng với mỗi phần tử trong chuỗi là một *hidden state* $h_t$, chứa các thông tin về các điểm bất kỳ trước đó trong chuỗi. \n",
    "\n",
    "Input trong Pytorch's LSTM là tensors 3 chiều. Chiều đầu tiên là chính bản thân cái chuỗi, chiều thứ hai đánh số (index) các trường hợp trong mini-batch, và chiều thứ 3 đánh số các phần tử của input.\n",
    "\n",
    "so lets just ignore that\n",
    "and assume we will always have just 1 dimension on the second axis. If\n",
    "we want to run the sequence model over the sentence \"The cow jumped\",\n",
    "our input should look like\n",
    "\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "   \\overbrace{q_\\text{The}}^\\text{row vector} \\\\\n",
    "   q_\\text{cow} \\\\\n",
    "   q_\\text{jumped}\n",
    "   \\end{bmatrix}\\end{align}\n",
    "\n",
    "Except remember there is an additional 2nd dimension with size 1.\n",
    "\n",
    "In addition, you could go through the sequence one at a time, in which\n",
    "case the 1st axis will have size 1 also.\n",
    "\n",
    "Let's see a quick example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f468c1cf710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tác giả: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input và output đều có 3 chiều: \n",
    "lstm = nn.LSTM(3, 3)  \n",
    "\n",
    "# tạo chuỗi có độ dài là 5:\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  \n",
    "\n",
    "# khởi tạo hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # lần lượt đi qua từng phần tử của chuỗi \n",
    "    # sau  mỗi bước, hidden được lưu trong hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta cũng có thể làm cho cả chuỗi cùng lúc. Giá trị đầu tiên được LSTM trả về là tất cả các  hidden states trong chuỗi. Cái thứ hài là hidden state gần nht (so sánh cái cuối cùng của out và hidden ở dưới, ta thấy chúng giống nhau).\n",
    "\n",
    "Đó là vì out cho ta tiếp cận tất cả các hidden states trong chuỗi. Hidden cho ta tiếp tục chuỗi và backpropagate, bằng cách pass nó như 1 argument vào lstm ở một thời điểm sau đó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0187,  0.1713, -0.2944]],\n",
      "\n",
      "        [[-0.3521,  0.1026, -0.2971]],\n",
      "\n",
      "        [[-0.3191,  0.0781, -0.1957]],\n",
      "\n",
      "        [[-0.1634,  0.0941, -0.1637]],\n",
      "\n",
      "        [[-0.3368,  0.0959, -0.0538]]], grad_fn=<CatBackward>)\n",
      "(tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<ViewBackward>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "# Thêm vào chiều thứ hai:\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "# clean out hidden state\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  \n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ví dụ: LSTM cho Part-of-Speech Tagging\n",
    "\n",
    "\n",
    "In this section, we will use an LSTM to get part of speech tags. We will\n",
    "not use Viterbi or Forward-Backward or anything like that, but as a\n",
    "(challenging) exercise to the reader, think about how Viterbi could be\n",
    "used after you have seen what is going on.\n",
    "\n",
    "The model is as follows: let our input sentence be\n",
    "$w_1, \\dots, w_M$, where $w_i \\in V$, our vocab. Also, let\n",
    "$T$ be our tag set, and $y_i$ the tag of word $w_i$.\n",
    "Denote our prediction of the tag of word $w_i$ by\n",
    "$\\hat{y}_i$.\n",
    "\n",
    "This is a structure prediction, model, where our output is a sequence\n",
    "$\\hat{y}_1, \\dots, \\hat{y}_M$, where $\\hat{y}_i \\in T$.\n",
    "\n",
    "To do the prediction, pass an LSTM over the sentence. Denote the hidden\n",
    "state at timestep $i$ as $h_i$. Also, assign each tag a\n",
    "unique index (like how we had word\\_to\\_ix in the word embeddings\n",
    "section). Then our prediction rule for $\\hat{y}_i$ is\n",
    "\n",
    "\\begin{align}\\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(Ah_i + b))_j\\end{align}\n",
    "\n",
    "That is, take the log softmax of the affine map of the hidden state,\n",
    "and the predicted tag is the tag that has the maximum value in this\n",
    "vector. Note this implies immediately that the dimensionality of the\n",
    "target space of $A$ is $|T|$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['The', 'dog', 'ate', 'the', 'apple'], ['DET', 'NN', 'V', 'DET', 'NN']),\n",
       " (['Everybody', 'read', 'that', 'book'], ['NN', 'V', 'DET', 'NN'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Everybody': 5, 'ate': 2, 'apple': 4, 'that': 7, 'read': 6, 'dog': 1, 'book': 8, 'the': 3, 'The': 0}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the \n",
    "# weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, \n",
    "                 vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM lấy word embeddings làm input và cho ra output \n",
    "        # là hidden states với số chiều là hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # layer tuyến tính maps từ không gian các \n",
    "        # hidden state sang tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Trước khi chạy, ta không có hidden state nên phải khởi tạo\n",
    "        # các chiều là (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train mô hình:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1389, -1.2024, -0.9693],\n",
      "        [-1.1065, -1.2200, -0.9834],\n",
      "        [-1.1286, -1.2093, -0.9726],\n",
      "        [-1.1190, -1.1960, -0.9916],\n",
      "        [-1.0137, -1.2642, -1.0366]])\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM,\n",
    "                   len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# phần tử ở vị trí i,j của output là score cho tag j cho word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for sentence, tags in training_data:\n",
    "        # clear các gradient của các lần chạy trước\n",
    "        model.zero_grad()\n",
    "\n",
    "        # clear các hidden state của LSTM,\n",
    "        # tách nó khỏi lịch sử của vòng lặp trước\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # biến input thành các tensor chứa các word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # chạy forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6973, -1.2696, -1.5089],\n",
      "        [-2.7420, -0.2178, -2.0302],\n",
      "        [-1.4045, -1.7772, -0.5354],\n",
      "        [-0.2387, -2.2666, -2.2193],\n",
      "        [-2.7822, -0.1865, -2.2232]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    # kết quả của dự đoán là tag có score cao nhất\n",
    "    # The sentence is \"the dog ate the apple\". i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
